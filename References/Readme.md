# References

1. Why RL : https://arxiv.org/abs/1805.09461
2. Attention Mechanism : https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/
3. RL on NLP : https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf
4. Seq2Seq Model : https://www.guru99.com/seq2seq-model.html
5. Seq2Seq : https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/
6. Bi-LSTM : https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0

### NLP
NLP or Natural Language Processing is one of the popular branches of Artificial Intelligence that helps computers understands, manipulate or respond to a human in their natural language. NLP is the engine behind Google Translate that helps us understand other languages.

### Encode Decoder

Just use a seq2seq model<br/>
 – encoder: reads the input one token at a time to build up its vector representation</br>
 – decoder: starts with encoder vector as context, then decodes one token at a time – feeding its own outputs back in to maintain a vector representation of what was produced so far</br>

Standard RNNs have trouble learning long distance dependencies. RNN, fixed length. 
Long Short-Term Memory (LSTM) combats this issue.

# Github Repos
1. https://github.com/himanshurawat443/Abstractive-Text-Summarization-using-seq-2-seq-RNN-s
2. https://github.com/theamrzaki/text_summurization_abstractive_methods
3. 

# Project Google Collab
1. https://colab.research.google.com/drive/1gGPIPGLowsAKl02c322-VGdW-5gPlBqW#scrollTo=o5DmlTIv9M1I

